---
title: "ifcb-pipeline"
author: "Anders Torstensson"
date: "`r Sys.Date()`"
params: 
  year: [2023, 2024]
  classifier: "Baltic"
  threshold: "opt"
  regional: TRUE # If output should only contain data from the classifier region
output: html_document
knit: (function(inputFile, encoding) {
                        rmarkdown::render(inputFile,
                        encoding=encoding, 
                        output_file=paste0("output/html_reports/ifcb_data_export_report_",
                        format(Sys.Date(), "%Y%m%d"),
                        ".html")) })
---

```{r setup, include=FALSE}
# Setting options for 'knitr' package chunks to display code in the output
knitr::opts_chunk$set(echo = TRUE)

# Start time
knit.time <- Sys.time()

# Load required libraries
library(iRfcb)
library(tidyverse)
library(worrms)
library(leaflet)
library(htmltools)
library(knitr)
library(patchwork)

# Installing the required Python dependencies in a specified virtual environment for the 'iRfcb' package
ifcb_py_install(envname = ".virtualenvs/iRfcb")

# Define paths from .Renviron
tlc_path <- Sys.getenv("tlc_path")
tk_path <- Sys.getenv("tk_path")

# Set TCL and TK PATH for knit
Sys.setenv(TCL_LIBRARY = tlc_path)
Sys.setenv(TK_LIBRARY = tk_path)
```

## IFCB data to SHARK

This document summarize data exports to SHARK from the Imaging FlowCytobot

```{r define_paths, echo=FALSE}
# Define paths from .Renviron
ifcb_path <- Sys.getenv("ifcb_path")
ferrybox_path <- Sys.getenv("ferrybox_path")

# Define class2use file
class2use_file <- file.path(ifcb_path, "config", paste0("class2use_", params$classifier, ".mat"))

# Define manual folder
manual_folder <- file.path(ifcb_path, "manual", params$classifier)

# Initialize list to store paths for each year
feature_folders <- list()
data_folders <- list()
class_folders <- list()
max_versions <- list()

# Loop through each year in params$year
for (year in params$year) {
  # Define paths for feature and HDR data
  feature_folder <- file.path(ifcb_path, "features", year)
  data_folder <- file.path(ifcb_path, "data", year)
  
  # Append to lists
  feature_folders <- c(feature_folders, feature_folder)
  data_folders <- c(data_folders, data_folder)

  # Define path for the generic class folder
  class_folder_generic <- file.path(ifcb_path, "classified", params$classifier)
  
  # List all class versions
  class_subfolders <- list.dirs(class_folder_generic, full.names = TRUE, recursive = FALSE)
  
  # Find all subfolders from the selected year
  class_subfolders <- class_subfolders[grepl(as.character(year), class_subfolders)]
  
  # Extract the version numbers from the subfolder names
  versions <- sub(paste0("class", year ,"_v"), "", basename(class_subfolders))
  versions <- as.numeric(versions)
  
  # Get the subfolder with the highest version number
  max_version <- max(versions, na.rm = TRUE)
  class_folder <- class_subfolders[which(versions == max_version)]
  
  # Append to list
  class_folders <- c(class_folders, class_folder)
  max_versions <- c(max_versions, max_version)
}

# Define results folders
output_folder <- "output"
```

## Read data

```{r read_hdr_data, echo=FALSE}
# Initialize a list to store the processed HDR data for each year
hdr_data_list <- list()
all_hdr_data_list <- list()

# Start time
start.time <- Sys.time()

for (i in seq_along(params$year)) {

  # Get current data folder
  data_folder <- data_folders[[i]]
  
  # Read HDR data from the specified data folder
  all_hdr_data <- ifcb_read_hdr_data(data_folder, gps_only = FALSE, verbose = FALSE)
  
  # Keep only GPS information
  hdr_data <- all_hdr_data %>%
    select(sample, gpsLatitude, gpsLongitude, timestamp, date, year, month, day, time, ifcb_number)
  
  # Identify rows in hdr_data where the year is not 2016 and GPS latitude or longitude is missing
  missing_position <- hdr_data %>%
    filter(!year == 2016) %>%
    filter(is.na(gpsLatitude)) %>%
    filter(is.na(gpsLongitude))
  
  # If there are rows with missing GPS positions
  if (nrow(missing_position) > 0) {
    # Retrieve ferrybox positions for the timestamps of the missing GPS data
    ferrybox_positions <- ifcb_get_svea_position(missing_position$timestamp, ferrybox_path)
  
    # Rename GPS latitude and longitude columns in ferrybox_positions to avoid conflicts
    ferrybox_positions <- ferrybox_positions %>%
      rename(gpsLatitude_fb = gpsLatitude,
             gpsLongitude_fb = gpsLongitude)
  
    # Merge hdr_data with ferrybox_positions based on timestamps
    hdr_data <- hdr_data %>%
      left_join(ferrybox_positions, by = "timestamp") %>%
      mutate(gpsLatitude = coalesce(gpsLatitude, gpsLatitude_fb),
             gpsLongitude = coalesce(gpsLongitude, gpsLongitude_fb)) %>%
      select(-gpsLongitude_fb, -gpsLatitude_fb)
  }
  
  # Store the processed hdr_data in the list
  hdr_data_list[[as.character(params$year[i])]] <- hdr_data
  
  # Store all the processed hdr_data in the list
  all_hdr_data_list[[as.character(params$year[i])]] <- all_hdr_data
}

# End time
end.time <- Sys.time()
runtime_hdr <- round(end.time - start.time, 2)
```

```{r get_biovolumes, echo=FALSE, include=FALSE}
# Start time
start.time <- Sys.time()

# Initialize a list to store the processed biovolume data for each year
biovolume_data_list <- list()

for (i in seq_along(params$year)) {
  cat("# Year:", params$year[i], "\n")
  
  # Get current folders
  data_folder <- data_folders[[i]]
  class_folder <- class_folders[[i]]
  feature_folder <- feature_folders[[i]]
  
  # Summarize biovolume data using IFCB data from the specified folders
  biovolume_data <- ifcb_summarize_biovolumes(
    feature_folder,
    class_folder,
    hdr_folder = data_folder,
    micron_factor = 1/3.4,
    diatom_class = "Bacillariophyceae",
    threshold = "opt"
  )
  
  # Store the summarized biovolume data in the list
  biovolume_data_list[[as.character(params$year[i])]] <- biovolume_data
}

# End time
end.time <- Sys.time()
runtime_biovolume <- round(end.time - start.time, 2)
```


```{r get_manual_data, echo=FALSE, include=FALSE}
# Start time
start.time <- Sys.time()

# Extract manual data
manual_data <- ifcb_count_mat_annotations(manual_folder,
                                          class2use_file,
                                          sum_level = "sample")
# Extract sample names
samples <- unique(manual_data$sample)

# Create empty dataframe
ml_analyzed <- data.frame()

# Get volume analyzed for all manual samples
for (i in seq_along(samples)) {
  hdr_file <- file.path(ifcb_path, 
                        "data", substr(samples[i], 2, 5), 
                        substr(samples[i], 1, 9), 
                        paste0(samples[i], ".hdr"))
  
  result <- ifcb_volume_analyzed(hdr_file)
  
  ml_analyzed_temp <- data.frame(sample = samples[i],
                                 ml_analyzed = result)
  
  ml_analyzed <- rbind(ml_analyzed, ml_analyzed_temp)
}

# Join manual data with volume data and calculate counts/l
manual_data <- manual_data %>%
  left_join(ml_analyzed) %>%
  rename(counts = n) %>%
  mutate(counts_per_liter = counts/ml_analyzed*1000)

# End time
end.time <- Sys.time()
runtime_manual <- round(end.time - start.time, 2)
```


## QC

```{r psd_check, echo=FALSE, include=FALSE}
# Start time
start.time <- Sys.time()

# Initialize a list to store PSD results for each year
psd_list <- list()

for (i in seq_along(params$year)) {

  # Get current data folder
  feature_folder <- feature_folders[[i]]
  data_folder <- data_folders[[i]]
  
  # Define result and plot folder paths
  psd_result_files <- file.path(ifcb_path, "psd", paste0("psd_", params$year[i]))
  psd_plot_folder <- file.path(ifcb_path, "psd", "figures", params$year[i])
  
  # Calculate the particle size distribution (PSD) using IFCB data from the specified folders
  psd <- ifcb_psd(feature_folder = feature_folder,
                  hdr_folder = data_folder,
                  save_data = TRUE,
                  output_file = psd_result_files,
                  plot_folder = psd_plot_folder,
                  use_marker = FALSE,
                  start_fit = 15,
                  r_sqr = 0.5,
                  beads = 10 ** 20,
                  bubbles = 150,
                  incomplete = c(1500, 3),
                  missing_cells = 0.7,
                  biomass = 1000,
                  bloom = 10,
                  humidity = 75,
                  micron_factor = 1/3.4)
  
  # Store PSD results in the list
  psd_list[[as.character(params$year[i])]] <- psd
}

# End time
end.time <- Sys.time()
runtime_psd <- round(end.time - start.time, 2)
```


```{r psd_print, echo=FALSE, include=TRUE, results='asis'}
for (i in seq_along(params$year)) {
  # Print PSD summary
  print(psd_list[[as.character(params$year[i])]]$flags %>%
          group_by(flag) %>%
          summarise("Number of samples" = n()) %>%
          arrange(desc(`Number of samples`)) %>%
          rename("Q-flag" = flag) %>%
          knitr::kable(caption = paste0("PSD Summary for Year ", 
                                       params$year[i], 
                                       " (total n samples: ", 
                                       nrow(psd_list[[as.character(params$year[i])]]$fits),
                                       ")")))
}
```


```{r coordinate_check, echo=FALSE}
# Initialize a list to store coordinate check results for each year
coordinate_check_list <- list()

for (i in seq_along(params$year)) {
  
  # Get current HDR data
  hdr_data <- hdr_data_list[[as.character(params$year[i])]]
  
  # Create a data frame with sample names and GPS coordinates from hdr_data
  positions <- data.frame(
    sample = sapply(strsplit(hdr_data$sample, "_"), `[`, 1),
    gpsLatitude = hdr_data$gpsLatitude,
    gpsLongitude = hdr_data$gpsLongitude) %>%
    filter(!is.na(gpsLatitude) | !is.na(gpsLongitude))
  
  # Determine if positions are near land using specified shapefile
  near_land <- ifcb_is_near_land(
    positions$gpsLatitude,
    positions$gpsLongitude,
    shape = "data/shapefiles/EEA_Coastline_Polygon_Shape/EEA_Coastline_20170228.shp")
  
  # Determine if positions are in the Baltic basin
  in_baltic <- ifcb_is_in_basin(
    positions$gpsLatitude,
    positions$gpsLongitude)
  
  # Add the near_land and in_baltic information to the positions data frame
  positions$near_land <- near_land
  positions$in_baltic <- in_baltic
  positions$basin <- ifcb_which_basin(positions$gpsLatitude, positions$gpsLongitude)
  
  # Store coordinate check results in the list
  coordinate_check_list[[as.character(params$year[i])]] <- positions
}
```

```{r q_flags, echo=FALSE, message=FALSE}
# Initialize lists to store results for each year
qflags_list <- list()
date_info_list <- list()

for (i in seq_along(params$year)) {

  # Get current PSD flags and positions data
  psd_flags <- psd_list[[as.character(params$year[i])]]$flags
  positions <- coordinate_check_list[[as.character(params$year[i])]]
  
  # Join psd$flags with positions data by "sample", add near_land_qflag, unite into flag, convert to sentence case, select sample and flag columns
  qflags <- psd_flags %>%
    full_join(positions, by = "sample") %>%
    mutate(near_land_qflag = ifelse(near_land, "Near land", NA)) %>%
    unite(col = flag, flag, near_land_qflag, na.rm = TRUE, sep = ", ") %>%
    mutate(flag = ifelse(str_to_sentence(flag) == "", NA, str_to_sentence(flag)),
           lon = gpsLongitude,
           lat = gpsLatitude) %>%
    select(sample, flag, lat, lon) %>%
    mutate(group = ifelse(is.na(flag), "blue", "red"))
  
  # Convert filenames in biovolume_data$sample to date information
  date_info <- ifcb_convert_filenames(qflags$sample)
  
  # Join qflags with date_info by "sample"
  qflags <- qflags %>%
    left_join(date_info, by = "sample")
  
  # Store qflags results in the list
  qflags_list[[as.character(params$year[i])]] <- qflags
  
  # Store date info in the list
  date_info_list[[as.character(params$year[i])]] <- date_info
}
```

```{r qc_maps, echo=FALSE, results='asis'}
# Initialize a list to store all qc_maps HTML
qc_maps_list <- list()

for (i in seq_along(params$year)) {

  # Get current Q-flags data for the year
  qflags <- qflags_list[[as.character(params$year[i])]]
  
  # Define the icon list with URLs for the markers
  sampleIcons <- iconList(
    blue = makeIcon(iconUrl = "https://raw.githubusercontent.com/pointhi/leaflet-color-markers/master/img/marker-icon-blue.png", iconWidth = 24, iconHeight = 32),
    red = makeIcon(iconUrl = "https://raw.githubusercontent.com/pointhi/leaflet-color-markers/master/img/marker-icon-red.png", iconWidth = 24, iconHeight = 32)
  )
  
  # Create maps for each month and display them
  month_names <- c("January", "February", "March", "April", "May", "June", "July", "August", "September", "October", "November", "December")
  
  for (j in 1:12) {
    gps_month <- qflags %>% filter(month(date) == j)
    
    if (nrow(gps_month) > 0) {
      map <- leaflet(data = gps_month) %>%
        addTiles() %>%
        addMarkers(
          lng = ~lon,
          lat = ~lat,
          icon = ~sampleIcons[group],
          popup = ~ifelse(is.na(flag), paste("Sample:", sample), paste("Sample:", sample, "<br>", "QFlag:", flag))
        )
      qc_maps_list <- c(qc_maps_list, paste(month_names[j], params$year[i]), list(map))
    }
  }
}
```

```{r render_qc_maps, include=TRUE, echo=FALSE}
# # Loop through each year
# for (i in seq_along(params$year)) {
#   cat("### Year:", params$year[i], "\n")
# 
#   tagList(qc_maps_list[[as.character(params$year[i])]])
# }

tagList(qc_maps_list)
```

```{r class_scores, echo=FALSE, message=FALSE}
# Initialize a list to store class_scores data frames for each year
class_scores_list <- list()
classifier_date_list <- list()

for (i in seq_along(params$year)) {

  # Get current class_folder and classifier
  class_folder <- class_folders[[i]]
  
  # List all files in the class_folder with full path names
  class_files <- list.files(class_folder, full.names = TRUE)
  
  # Extract the classifier name from the first file in class_files
  classifier <- ifcb_get_mat_variable(class_files[1], "classifierName")[1]
  
  # Get date of classification
  class_date <- date(file.info(class_files[1])$mtime)
  
  # Construct the class_score_file name based on params$threshold and classifier
  class_score_file <- gsub(".mat", ifelse(params$threshold == "opt" | params$threshold == "adhoc",
                                          paste0("_", params$threshold, ".csv"), 
                                          ""), 
                           classifier)
  
  # Remap path
  if (!ifcb_path == "Z:/data") {
    class_score_file <- gsub("Z:\\\\data", ifcb_path, class_score_file)
  }
  
  # Check if the class_score_file exists
  if (file.exists(class_score_file)) {
    # Read the class scores from the CSV file
    class_scores <- read_csv(class_score_file,
                             show_col_types = FALSE)
  } else {
    # Create a data frame with unique class names and NA values for precision, detection_probability, and miss_probability
    class_scores <- data.frame(class = unique(biovolume_data$class),
                               precision = NA,
                               detection_probability = NA,
                               miss_probability = NA)
  }
  
  # Store class_scores in the list
  class_scores_list[[as.character(params$year[i])]] <- class_scores
  
  # Store class_scores in the list
  classifier_date_list[[as.character(params$year[i])]] <- class_date
}
```

```{r class_names, echo=FALSE, message=FALSE}
# Initialize a list to store class_names data frames for each year
class_names_list <- list()

for (i in seq_along(params$year)) {
  
  # Get current class_scores and biovolume_data for the year
  class_scores <- class_scores_list[[as.character(params$year[i])]]
  biovolume_data <- biovolume_data_list[[as.character(params$year[i])]]
  
  # Extract unique taxa names from biovolume_data$class
  taxa_names <- unique(biovolume_data$class)
  
  # Clean taxa_names by substituting specific patterns with spaces or empty strings
  taxa_names_clean <- gsub("_", " ", taxa_names)
  taxa_names_clean <- gsub(" single cell", "", taxa_names_clean)
  taxa_names_clean <- gsub(" chain", "", taxa_names_clean)
  taxa_names_clean <- gsub(" coil", "", taxa_names_clean)
  taxa_names_clean <- gsub(" filament", "", taxa_names_clean)
  taxa_names_clean <- gsub("-like", "", taxa_names_clean)
  taxa_names_clean <- gsub(" like", "", taxa_names_clean)
  taxa_names_clean <- gsub(" bundle", "", taxa_names_clean)
  taxa_names_clean <- gsub(" larger than 30unidentified", "", taxa_names_clean)
  taxa_names_clean <- gsub(" smaller than 30unidentified", "", taxa_names_clean)
  taxa_names_clean <- gsub(" smaller than 30", "", taxa_names_clean)
  taxa_names_clean <- gsub("\\<cf\\>", "", taxa_names_clean)
  taxa_names_clean <- gsub("\\<spp\\>", "", taxa_names_clean)
  taxa_names_clean <- gsub("\\<sp\\>", "", taxa_names_clean)
  taxa_names_clean <- gsub(" group", "", taxa_names_clean)
  taxa_names_clean <- gsub("  ", " ", taxa_names_clean)
  taxa_names_clean <- gsub(" ([A-Z])", "/\\1", taxa_names_clean)
  
  taxa_names_clean <- trimws(taxa_names_clean)
  
  # Retrieve WoRMS records for taxa_names_clean, including non-marine records
  worms_records <- wm_records_names(taxa_names_clean, marine_only = FALSE)
  
  # Extract Aphia IDs from worms_records
  aphia_id <- sapply(worms_records, iRfcb:::extract_aphia_id)
  classes <- sapply(worms_records, iRfcb:::extract_class)
  
  # Create class_names data frame with taxa information
  class_names <- data.frame(class = taxa_names,
                            class_clean = taxa_names_clean,
                            aphia_id,
                            worms_class = classes,
                            sflag = ifelse(grepl("-like", taxa_names) | grepl("_cf_", taxa_names)| grepl("_like", taxa_names), "CF", NA),
                            is_diatom = ifcb_is_diatom(taxa_names_clean)) %>%
    mutate(sflag = ifelse(grepl("\\<spp\\>", gsub("_", " ", taxa_names)), paste(ifelse(is.na(sflag), "", sflag), "SPP"), sflag)) %>%
    mutate(sflag = ifelse(grepl("\\<group\\>", gsub("_", " ", taxa_names)), paste(ifelse(is.na(sflag), "", sflag), "GRP"), sflag)) %>%
    mutate(sflag = ifelse(grepl("\\<sp\\>", gsub("_", " ", taxa_names)), paste(ifelse(is.na(sflag), "", sflag), "SP"), sflag)) %>%
    mutate(trophic_type = ifcb_get_trophic_type(class_clean)) %>%
    left_join(class_scores_list[[as.character(params$year[i])]], by = "class")
  
  # Store class_names in the list
  class_names_list[[as.character(params$year[i])]] <- class_names
}
```


```{r print_class_names, echo=FALSE, include=TRUE, results='asis'}
# Loop through each year
for (i in seq_along(params$year)) {
  # Print the class_names data frame for the current year
  print(class_names_list[[as.character(params$year[i])]] %>%
          select(-worms_class, -precision, -detection_probability, -miss_probability) %>%
          arrange(class) %>%
          knitr::kable(caption = paste("Class Summary for Year", params$year[i])))
}
```


```{r merge_data, echo=FALSE, message=FALSE}
# Initialize a list to store aggregated data frames for each year
data_aggregated_list <- list()

for (i in seq_along(params$year)) {

  # Extract relevant data frames for the current year
  biovolume_data <- biovolume_data_list[[as.character(params$year[i])]]
  date_info <- hdr_data_list[[as.character(params$year[i])]] %>%
    mutate(sample = sapply(strsplit(sample, "_"), `[`, 1)) %>%
    select(-gpsLatitude, -gpsLongitude)
  qflags_select <- select(qflags_list[[as.character(params$year[i])]], sample, flag)
  positions <- coordinate_check_list[[as.character(params$year[i])]]
  class_names <- class_names_list[[as.character(params$year[i])]]

  # Perform left joins to combine data from multiple sources for the current year
  data <- biovolume_data %>%
    mutate(sample = sapply(strsplit(sample, "_"), `[`, 1)) %>%
    left_join(date_info, by = "sample") %>%
    left_join(qflags_select, by = "sample") %>%
    left_join(positions, by = "sample") %>%
    left_join(class_names, by = "class") %>%
    mutate(verification = "None")
  
  # Perform left to combine manual data from multiple sources for the current year
  data_manual <- manual_data %>%
    mutate(sample = sapply(strsplit(sample, "_"), `[`, 1)) %>%
    left_join(date_info, by = "sample") %>%
    filter(year == params$year[i]) %>%
    left_join(positions, by = "sample") %>%
    left_join(class_names, by = "class") %>%
    select(-precision, -detection_probability, -miss_probability) %>%
    mutate(verification = "Human")
  
  # Aggregate multiple classes of the same taxa
  data_aggregated <- data %>%
    group_by(sample, ml_analyzed, gpsLatitude, gpsLongitude, near_land,
             in_baltic, basin, worms_class, timestamp, date, year, month, day, time, ifcb_number, 
             class_clean, aphia_id, sflag, trophic_type, is_diatom, flag, verification) %>%
    summarise(counts = sum(counts, na.rm = TRUE),
              biovolume_mm3 = sum(biovolume_mm3, na.rm = TRUE),
              carbon_ug = sum(carbon_ug, na.rm = TRUE),
              counts_per_liter = sum(counts_per_liter, na.rm = TRUE),
              biovolume_mm3_per_liter = sum(biovolume_mm3_per_liter, na.rm = TRUE),
              carbon_ug_per_liter = sum(carbon_ug_per_liter, na.rm = TRUE),
              class = str_c(class, collapse = ", "),
              precision = str_c(precision, collapse = ", "),
              detection_probability = str_c(detection_probability, collapse = ", "),
              miss_probability = str_c(miss_probability, collapse = ", "),
              .groups = "drop"
    )
  
  # Combine human verified data (manual annotations) with automatic classification
  data_aggregated <- bind_rows(data_aggregated, data_manual)
  
  # Filter data from the classifier region, when specified in params
  if (params$regional) {
    if (params$classifier == "Baltic") {
      data_aggregated_test <- data_aggregated %>%
        filter(in_baltic)
    } else {
      data_aggregated_test <- data_aggregated %>%
        filter(!in_baltic)
    }
  }
  
  # Store aggregated data for the current year in the list
  data_aggregated_list[[as.character(params$year[i])]] <- data_aggregated
}
```

## Monthly Mean Counts per Liter by Basin

With and without flagged data, per class

```{r plot_data, echo=FALSE, message=FALSE}
# Initialize a list to store plots for each year
plots <- list()

for (i in seq_along(params$year)) {

  # Extract data for the current year
  data_aggregated_year <- data_aggregated_list[[as.character(params$year[i])]]
  
  # Get unique worms classes
  worms_classes <- unique(data_aggregated_year$worms_class)
  
  # Initialize a list to store plots for the current year and worms class
  year_plots <- list()
  
  for (worms_class in worms_classes) {
    
    # Filter data for the current worms class
    data_worms_class <- data_aggregated_year %>%
      filter(worms_class == !!worms_class)
    
    # Calculate monthly means and plot for all data
    plot_all <- data_worms_class %>%
      group_by(basin, month) %>%
      summarise(mean = mean(counts_per_liter, na.rm = TRUE),
                sd = sd(counts_per_liter, na.rm = TRUE),
                .groups = "drop") %>%
      ggplot(aes(x = month, y = mean, color = basin)) +
      geom_point() +
      geom_errorbar(aes(ymin = mean - sd, ymax = mean + sd), width = 0.2) +
      labs(title = paste0(params$year[i], ", ", worms_class, ", all data"),
           x = "Month",
           y = "Counts per Liter",
           color = "Basin") +
      scale_x_discrete(limits = as.character(1:12)) +  # Set x-axis limits to all month names
      theme_minimal() +
      theme(legend.position = "bottom")
    
    # Calculate monthly means and plot for data without Q-flag
    plot_no_flag <- data_worms_class %>%
      filter(is.na(flag)) %>%
      group_by(basin, month) %>%
      summarise(mean = mean(counts_per_liter, na.rm = TRUE),
                sd = sd(counts_per_liter, na.rm = TRUE),
                .groups = "drop") %>%
      ggplot(aes(x = month, y = mean, color = basin)) +
      geom_point() +
      geom_errorbar(aes(ymin = mean - sd, ymax = mean + sd), width = 0.2) +
      labs(title = paste0(worms_class, ", no flagged data"),
           x = "Month",
           y = "Counts per Liter",
           color = "Basin") +
      scale_x_discrete(limits = as.character(1:12)) +  # Set x-axis limits to all month names
      theme_minimal() +
      theme(legend.position = "bottom")
    
    # Combine the two plots with a shared legend
    combined_plot <- plot_all + plot_no_flag + plot_layout(guides = "collect") & theme(legend.position = "bottom")
    
    # Store combined plot for the current worms class
    year_plots[[worms_class]] <- combined_plot
  }
  
  # Store all worms_class plots for the current year
  plots[[as.character(params$year[i])]] <- year_plots
}

# Print combined plots for each year and each worms_class
for (i in seq_along(params$year)) {
  for (worms_class in names(plots[[as.character(params$year[i])]])) {
    print(plots[[as.character(params$year[i])]][[worms_class]])
  }
}

```

# SHARK export

```{r format_shark, echo=FALSE, message=FALSE}
# Initialize a list to store shark_df for each year
shark_dfs <- list()

for (i in seq_along(params$year)) {

  # Extract data for the current year
  data_aggregated_year <- data_aggregated_list[[as.character(params$year[i])]]
  
  # Retrieve column names for Shark database integration
  shark_col <- ifcb_get_shark_colnames()
  
  # Create a data frame with empty rows matching the length of data
  shark_col[1:nrow(data_aggregated_year),] <- ""
  
  # Create shark_df by mapping relevant data from 'data' to Shark database columns
  shark_df <- shark_col %>% 
    mutate(MYEAR = data_aggregated_year$year, # YEAR
           STATN = data_aggregated_year$sample,
           SAMPLING_PLATFORM = data_aggregated_year$ifcb_number, # can skip if not needed
           PROJ = NA,
           ORDERER = "SMHI", 
           SHIPC = "77SE",
           CRUISE_NO = NA,
           SDATE = data_aggregated_year$date, # DATE Switch this to YYMM
           DATE_TIME = format(data_aggregated_year$timestamp, "%Y%m%d%H%M%S"), # YYYYMMDDHHMMSS FORMAT
           TIMEZONE = "UTC",
           STIME = data_aggregated_year$time,
           LATIT = data_aggregated_year$gpsLatitude,
           LONGI = data_aggregated_year$gpsLongitude,
           POSYS = "GPS",
           MSTAT = NA,
           PDMET = "Mixed surface layer",
           METFP = "None",
           IFCBNO	= data_aggregated_year$ifcb_number,
           MPROG = "PROJ",
           MNDEP = 4,
           MXDEP = 4,
           SLABO = "SMHI",
           ACKR_SMP = "N",
           SMTYP = NA,
           SMVOL = data_aggregated_year$ml_analyzed, # VOLUME
           SMPNO = data_aggregated_year$sample, # SAMPLE NAME
           LATNM = data_aggregated_year$class_clean, # SPECIES
           SFLAG = data_aggregated_year$sflag, # SP or SPP
           TRPHY = data_aggregated_year$trophic_type,
           APHIA_ID = data_aggregated_year$aphia_id,
           IMAGE_VERIFICATION = data_aggregated_year$verification,
           CLASS_NAME = data_aggregated_year$class,
           CLASS_PD = data_aggregated_year$detection_probability,
           CLASS_PR = data_aggregated_year$precision,
           CLASS_PM = data_aggregated_year$miss_probability,
           COUNT = data_aggregated_year$counts, # COUNTS per SAMPLE
           COEFF = 1000/data_aggregated_year$ml_analyzed,
           ABUND = data_aggregated_year$counts_per_liter, #COUNTS PER LITER
           QFLAG = data_aggregated_year$flag,
           C_CONC = data_aggregated_year$carbon_ug_per_liter, # CARBON PER LITER
           BIOVOL = data_aggregated_year$biovolume_mm3_per_liter, #BIOVOLUME PER LITER
           METOA = NA,
           COUNTPROG = "MATLAB", 
           ALABO = "SMHI",
           ACKR_ANA = "N",
           ANADATE = classifier_date_list[[as.character(params$year[i])]], # ANALYSIS DATE -GET THIS FROM THE SAVE DATE ON THE MATLAB FILE?
           METDC = paste("https://github.com/hsosik/ifcb-analysis",
                         "https://github.com/kudelalab/PSD", 
                         "https://github.com/EuropeanIFCBGroup/iRfcb",
                         sep = ", "), # METHOD
           TRAINING_SET_ANNOTATED_BY = "Ann-Turi Skjevik",
           TRAINING_SET = "https://doi.org/10.17044/scilifelab.25883455.v3",
           CLASSIFIER_CREATED_BY = "Anders Torstensson",
           CLASSIFIER_USED = paste0(params$classifier, " v.", max_versions[[i]]),
           MANUAL_QC_DATE = NA,
           PRE_FILTER_SIZE ="150" # unit um
    ) %>%
    mutate(CLASSIFIER_CREATED_BY = ifelse(IMAGE_VERIFICATION == "Human", NA, CLASSIFIER_CREATED_BY),
           CLASSIFIER_USED = ifelse(IMAGE_VERIFICATION == "Human", NA, CLASSIFIER_USED)) %>%
    filter(!COUNT == 0)
  
  # Store shark_df for the current year in the list
  shark_dfs[[as.character(params$year[i])]] <- shark_df
}

# Concatenate all shark_dfs into a single data frame
shark_df_combined <- do.call(rbind, shark_dfs)
```


```{r data_delivery, echo=FALSE, message=FALSE}
# Initialize lists to store paths and delivery note contents for each year
processed_data_paths <- list()
received_data_paths <- list()
delivery_notes <- list()
data_delivery_paths <- list()

# Loop through each year
for (i in seq_along(params$year)) {
  # Define data deliviery path
  data_delivery_path <- file.path(output_folder, paste0("SHARK_IFCB_", as.character(params$year[i]), "_", params$classifier, "_SMHI"))
  
  # Define paths for processed, received, and correspondence data folders for current year
  processed_data <- file.path(data_delivery_path, "processed_data")
  received_data <- file.path(data_delivery_path, "received_data")
  correspondence <- file.path(data_delivery_path, "correspondence")
  
  # Create directories if they do not exist
  if (!dir.exists(processed_data)) {
    dir.create(processed_data, recursive = TRUE)
  }
  
  if (!dir.exists(received_data)) {
    dir.create(received_data, recursive = TRUE)
  }
  
  if (!dir.exists(correspondence)) {
    dir.create(correspondence, recursive = TRUE)
  }
  
  # Save shark_df data to a tab-delimited file in processed_data folder for current year
  shark_df_year <- shark_dfs[[as.character(params$year[i])]] # Retrieve shark_df for current year
  write_tsv(shark_df_year, file = file.path(processed_data, "data.txt"), na = "") # Save as tab-delimited file
  
  # Save shark_df data to a tab-delimited file in received_data folder with dynamic filename
  filename <- paste0("shark_data_", as.character(params$year[i]), "_", tolower(params$classifier), "_", Sys.Date(), ".txt")
  write_tsv(shark_df_year, file = file.path(received_data, filename), na = "") # Save as tab-delimited file
  
  # Define delivery note content as a character vector for current year
  delivery_note_content <- c(
    paste("provtagningsår:", params$year[i]),
    "datatyp: IFCB",
    "rapporterande institut: SMHI",
    paste("rapporteringsdatum:", Sys.Date()),
    "kontaktperson: Anders Torstensson",
    "format: Phytoplankton:PP_SMHI",
    "data kontrollerad av: Leverantör",
    "övervakningsprogram:",
    "beställare: SMHI",
    "projekt:",
    "kommentarer:",
    "status: test"
  )
  
  # Write delivery note content to a .txt file in processed_data folder for current year
  writeLines(delivery_note_content, file.path(processed_data, "delivery_note.txt"))
  
  # Store paths and delivery notes for current year
  data_delivery_paths[[as.character(params$year[i])]] <- data_delivery_path
}

# Print paths and confirmation message for each year
cat("Data delivery saved in", output_folder, "\n")
for (i in seq_along(params$year)) {
  cat("Year:", params$year[i], "- data package:", data_delivery_paths[[as.character(params$year[i])]], "\n")
}

end.time <- Sys.time()
runtime_knit <- round(end.time - knit.time, 2)
```

# Summarise runtimes
```{r api_operation_summary, echo=FALSE}
runtime_variables <- c("running the whole script",
                       "extracting HDR data",
                       "extracting count and biovolume data",
                       "extracting manual count data",
                       "analysing PSD")

runtime_values <- c(runtime_knit, runtime_hdr, runtime_biovolume, runtime_manual, runtime_psd)

for (i in seq_along(runtime_variables)) {
  cat("Time taken for ", runtime_variables[i], ": ", round(runtime_values[i]/3600, 2), "h", "\n", sep = "")
}
```

# Reproducibility

```{r reproducibility, echo=FALSE}
# Date time
cat("Time started:", format(knit.time), "\n")
cat("Time finished:", format(Sys.time()), "\n")

# Here we store the session info for this script
sessioninfo::session_info()
```